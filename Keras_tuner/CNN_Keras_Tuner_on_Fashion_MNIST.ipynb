{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN - Keras Tuner on Fashion MNIST",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utQn23MnsclN",
        "colab_type": "text"
      },
      "source": [
        "# Using Keras_Tuner to tune the hyper parameters of DL models\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MbcoqQ4DcqgB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f188543c-4110-496f-e51a-9f69f6baa56d"
      },
      "source": [
        "# imports\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag-QD042b88D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "a496b696-63a8-422a-ed5c-5ec522d16a69"
      },
      "source": [
        "# install keras tuner\n",
        "!pip install -U keras-tuner"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-tuner\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/f7/4b41b6832abf4c9bef71a664dc563adb25afc5812831667c6db572b1a261/keras-tuner-1.0.1.tar.gz (54kB)\n",
            "\r\u001b[K     |██████                          | 10kB 22.9MB/s eta 0:00:01\r\u001b[K     |████████████                    | 20kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 30kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 40kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 51kB 3.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 3.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.18.3)\n",
            "Requirement already satisfied, skipping upgrade: tabulate in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.8.7)\n",
            "Collecting terminaltables\n",
            "  Downloading https://files.pythonhosted.org/packages/9b/c4/4a21174f32f8a7e1104798c445dacdc1d4df86f2f26722767034e4de4bff/terminaltables-3.1.0.tar.gz\n",
            "Collecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (4.38.0)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner) (0.22.2.post1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2020.4.5.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->keras-tuner) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->keras-tuner) (0.14.1)\n",
            "Building wheels for collected packages: keras-tuner, terminaltables\n",
            "  Building wheel for keras-tuner (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-cp36-none-any.whl size=73200 sha256=44968692e005610c1c6f8b939a5aaa28e700554472a0a77cc60bd7a6c99efbbe\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/cc/62/52716b70dd90f3db12519233c3a93a5360bc672da1a10ded43\n",
            "  Building wheel for terminaltables (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for terminaltables: filename=terminaltables-3.1.0-cp36-none-any.whl size=15356 sha256=3401f94aaa896bf5e1ba29321b5ecd62de23c007ab3dc9ce7e1a820e79f0a6bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/30/6b/50/6c75775b681fb36cdfac7f19799888ef9d8813aff9e379663e\n",
            "Successfully built keras-tuner terminaltables\n",
            "Installing collected packages: terminaltables, colorama, keras-tuner\n",
            "Successfully installed colorama-0.4.3 keras-tuner-1.0.1 terminaltables-3.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyjofUp3cjBx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "9688aee6-9301-4195-c605-414f85f9e7c7"
      },
      "source": [
        "# download the fashion MNIST data\n",
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "(train_images, train_label), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "# rescale images betwee 0-1\n",
        "train_images = train_images/255\n",
        "test_images = test_images/255"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7frG3SgAiE3b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ffd1177d-583d-4298-db77-18e10ac2bc16"
      },
      "source": [
        "print(train_images.shape, len(train_images), test_images.shape, len(test_images))\n",
        "\n",
        "# reshape the images to 28,28,1\n",
        "train_images = train_images.reshape(len(train_images), 28, 28, 1)\n",
        "test_images = test_images.reshape(len(test_images), 28, 28, 1)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) 60000 (10000, 28, 28) 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzzhDzxQdPMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# build a CNN model using keras tuner\n",
        "\n",
        "def build_model(hp):\n",
        "  model = tf.keras.models.Sequential([\n",
        "      # first conv layer\n",
        "      tf.keras.layers.Conv2D(\n",
        "          filters=hp.Int('conv1_filter', min_value=32, max_value=128, step=16),\n",
        "          kernel_size=hp.Choice('conv1_kernel', values=[3,5]),\n",
        "          activation='relu',\n",
        "          input_shape=(28, 28, 1)),\n",
        "\n",
        "      # second conv layer\n",
        "      tf.keras.layers.Conv2D(\n",
        "          filters=hp.Int('conv2_filter', min_value=32, max_value=64, step=16),\n",
        "          kernel_size=hp.Choice('conv2_kernel', values=[3,5]),\n",
        "          activation='relu'),\n",
        "\n",
        "      # flatten the output\n",
        "      tf.keras.layers.Flatten(),\n",
        "\n",
        "      # Dense layer\n",
        "      tf.keras.layers.Dense(\n",
        "          units=hp.Int(\"Dense1_units\", min_value=32, max_value=128, step=16),\n",
        "          activation='relu'),\n",
        "      \n",
        "      # softmax layer\n",
        "      tf.keras.layers.Dense(10, activation='softmax')])\n",
        "  \n",
        "  model.compile(\n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=hp.Choice(\"Alearning_rate\", [1e-4, 1e-3, 1e-2, 1e-1, 3e-1])),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['accuracy'])\n",
        "\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKMPvgMqgmVr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "2217164a-31c4-4be8-f5f9-c7fa2f6adb26"
      },
      "source": [
        "from kerastuner import RandomSearch\n",
        "from kerastuner.engine.hyperparameters import HyperParameters\n",
        "\n",
        "# create a random_search object to do trials\n",
        "tune = RandomSearch(hypermodel=build_model, objective='val_accuracy', max_trials=5, \n",
        "                    project_name='fashion MNIST', directory='output')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Reloading Oracle from existing project output/fashion MNIST/oracle.json\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
            "INFO:tensorflow:Reloading Tuner from output/fashion MNIST/tuner0.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CakbLkqchvMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "65b30c65-3435-4946-9a2a-a0503daf483e"
      },
      "source": [
        "# begin the trials\n",
        "tune.search(train_images, train_label, epoch=3, validation_split=0.2)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Oracle triggered exit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1V21KY6ZmNqG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "39c01479-4f82-4bff-d9c7-315eb286bccf"
      },
      "source": [
        "tuned_model = tune.get_best_models(num_models=1)[0]\n",
        "tuned_model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 26, 26, 112)       1120      \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 24, 24, 48)        48432     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 27648)             0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 144)               3981456   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1450      \n",
            "=================================================================\n",
            "Total params: 4,032,458\n",
            "Trainable params: 4,032,458\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QO-HWmNmpAO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "outputId": "2a453def-9a99-4924-efa2-effb6932897d"
      },
      "source": [
        "# we will use the best tuned model to train now\n",
        "history = tuned_model.fit(train_images, train_label, epochs=10, validation_split=0.1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1688/1688 [==============================] - 8s 4ms/step - loss: 0.2392 - accuracy: 0.9111 - val_loss: 0.2601 - val_accuracy: 0.9053\n",
            "Epoch 2/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.1694 - accuracy: 0.9381 - val_loss: 0.2467 - val_accuracy: 0.9115\n",
            "Epoch 3/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.1131 - accuracy: 0.9580 - val_loss: 0.2488 - val_accuracy: 0.9133\n",
            "Epoch 4/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0754 - accuracy: 0.9725 - val_loss: 0.3277 - val_accuracy: 0.9163\n",
            "Epoch 5/10\n",
            "1688/1688 [==============================] - 8s 4ms/step - loss: 0.0514 - accuracy: 0.9811 - val_loss: 0.3799 - val_accuracy: 0.9100\n",
            "Epoch 6/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0357 - accuracy: 0.9879 - val_loss: 0.4434 - val_accuracy: 0.9090\n",
            "Epoch 7/10\n",
            "1688/1688 [==============================] - 8s 4ms/step - loss: 0.0277 - accuracy: 0.9902 - val_loss: 0.4384 - val_accuracy: 0.9133\n",
            "Epoch 8/10\n",
            "1688/1688 [==============================] - 8s 4ms/step - loss: 0.0239 - accuracy: 0.9916 - val_loss: 0.5221 - val_accuracy: 0.9090\n",
            "Epoch 9/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0190 - accuracy: 0.9934 - val_loss: 0.5178 - val_accuracy: 0.9103\n",
            "Epoch 10/10\n",
            "1688/1688 [==============================] - 7s 4ms/step - loss: 0.0165 - accuracy: 0.9949 - val_loss: 0.5794 - val_accuracy: 0.9145\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6y45MH4nT4H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "cf8afa84-9e85-4078-a719-431315fbb86b"
      },
      "source": [
        "train_acc = tuned_model.evaluate(train_images, train_label)\n",
        "test_acc = tuned_model.evaluate(test_images, test_labels)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0685 - accuracy: 0.9878\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.6337 - accuracy: 0.9089\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}